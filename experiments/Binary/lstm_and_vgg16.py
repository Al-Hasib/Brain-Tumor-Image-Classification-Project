# -*- coding: utf-8 -*-
"""lstm and vgg16.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P1cDi790erqdoxcfE6efHaGMcYUCVR-5
"""

import warnings
warnings.filterwarnings('ignore')

!unzip /content/drive/MyDrive/brain_mri_dataset.zip

import numpy as np
import matplotlib.pyplot as plt
import os
import math
import shutil
import glob

# Counting the number of class from the dataset
Root_data = "/content/brain_mri_dataset"
number_of_image = {}
for dir in os.listdir(Root_data):
    number_of_image[dir] = len(os.listdir(os.path.join(Root_data, dir)))

number_of_image.items()

# Splitting the dataset into train, test, and validation

# creating a function for training, testing and validating
def datafolder(p, split):
    if not os.path.exists("./" + p):
        os.mkdir("./" + p)
        for dir in os.listdir(Root_data):
            os.makedirs("./" + p + "/" + dir)
            for img in np.random.choice(a=os.listdir(os.path.join(Root_data, dir)),
                                        size=(math.floor(split * number_of_image[dir]) - 2), replace=False):
                O = os.path.join(Root_data, dir, img)
                D = os.path.join("./" + p, dir)
                shutil.copy(O, D)
                os.remove(O)
    else:
        print(f"{p} folder exists")

datafolder("train", 0.7)

datafolder("test", 0.15)

datafolder("valid", 0.15)

Root_data = "/content/valid"
number_of_image = {}
for dir in os.listdir(Root_data):
    number_of_image[dir] = len(os.listdir(os.path.join(Root_data, dir)))
number_of_image.items()

# Data preprocessing
from keras.preprocessing.image import ImageDataGenerator

def preprocessingimg1(path):
    image_data = ImageDataGenerator(zoom_range=0.2, shear_range=0.2, rescale=1/255, horizontal_flip=True)
    image = image_data.flow_from_directory(directory=path, target_size=(224, 224), batch_size=16, class_mode='binary')
    return image

path = "/content/train"
train_data = preprocessingimg1(path)

def preprocessingimg2(path):
    image_data = ImageDataGenerator(rescale=1/255)
    image = image_data.flow_from_directory(directory=path, target_size=(224, 224), batch_size=32, class_mode='binary')
    return image

path = "/content/test"
test_data = preprocessingimg2(path)

path = "/content/valid"
valid_data = preprocessingimg2(path)

# Feature extraction using VGG16
from keras.applications import VGG16
from keras.models import Model
from keras.layers import GlobalAveragePooling2D

# Load VGG16 model
base_model_vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze base model layers
for layer in base_model_vgg16.layers:
    layer.trainable = False

# Add custom layers for feature extraction
x = base_model_vgg16.output
x = GlobalAveragePooling2D()(x)
feature_extractor_model = Model(inputs=base_model_vgg16.input, outputs=x)

# Extract features from the images
def extract_features(data_generator, model):
    features = []
    labels = []
    for i in range(len(data_generator)):
        x, y = data_generator[i]
        feature = model.predict(x)
        features.append(feature)
        labels.append(y)
    return np.vstack(features), np.hstack(labels)

train_features, train_labels = extract_features(train_data, feature_extractor_model)
valid_features, valid_labels = extract_features(valid_data, feature_extractor_model)
test_features, test_labels = extract_features(test_data, feature_extractor_model)

# Reshape the features to be suitable for LSTM input
train_features = train_features.reshape(train_features.shape[0], 1, train_features.shape[1])
valid_features = valid_features.reshape(valid_features.shape[0], 1, valid_features.shape[1])
test_features = test_features.reshape(test_features.shape[0], 1, test_features.shape[1])

# LSTM model building
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout

model_lstm = Sequential()
model_lstm.add(LSTM(128, input_shape=(1, train_features.shape[2]), return_sequences=True))
model_lstm.add(Dropout(0.5))
model_lstm.add(LSTM(64, return_sequences=False))
model_lstm.add(Dropout(0.5))
model_lstm.add(Dense(1, activation='sigmoid'))

model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model_lstm.summary()

# Early stopping and model checkpoint
from keras.callbacks import ModelCheckpoint, EarlyStopping

es = EarlyStopping(monitor="val_accuracy", min_delta=0.01, patience=5, verbose=1, mode='auto')
mc = ModelCheckpoint(monitor="val_accuracy", filepath="./cancer_detection_lstm_model.h5", verbose=1, save_best_only=True, mode='auto')
cd = [es, mc]

# Train the LSTM model
hs = model_lstm.fit(
    train_features, train_labels,
    epochs=30,
    batch_size=16,
    validation_data=(valid_features, valid_labels),
    callbacks=cd
)

from keras.models import load_model
# Load the best model
model = load_model("./cancer_detection_lstm_model.h5")

# Evaluate the model
accuracy = model.evaluate(test_features, test_labels)[1]
print(f"The accuracy of the LSTM model is {accuracy * 100}%")